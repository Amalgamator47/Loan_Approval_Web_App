# -*- coding: utf-8 -*-
"""Loan_Approval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D822di834KzireFf14pktgjFfrNXQRv0
"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np
import pandas as pd
import seaborn as sn

pip install streamlit

import streamlit as st

data=pd.read_csv("/content/train_u6lujuX_CVtuZ9i (1).csv")

data.head()

data.columns

Col=["Loan_ID","Gender","Married,Dependents","Education","Self_Employed","ApplicantIncome","CoapplicantIncome","LoanAmount","Loan_Amount_Term","Credit_History","Property_Area","Loan_Status"]

for i in Col:
  print(i)

data.head()

data.drop(columns=["Loan_ID"],axis=1,inplace=True)

data.head()

data.info()



data.info()

data.isnull().sum()
data.dropna(inplace=True)

data.drop(columns=['Gender'], inplace=True)

data['Married'] = data['Married'].map({'Yes': 1, 'No': 0})

data['Dependents'].value_counts()

data['Dependents'] = data['Dependents'].map({'1': 1, '0': 0,'2':2,'3+':3})

data['Education'].value_counts()

data['Education'] = data['Education'].map({'Graduate': 1, 'Not Graduate': 0})

data['Self_Employed'].value_counts()

data['Self_Employed'] = data['Self_Employed'].map({'Yes': 1, 'No': 0})

data['Property_Area'].value_counts()

data['Property_Area'] = data['Property_Area'].map({'Semiurban': 1, 'Urban': 0,'Rural':2})

data['Loan_Status'].value_counts()

data['Loan_Status'] = data['Loan_Status'].map({'Y': 1, 'N': 0})

data['Loan_Status'].value_counts()



"""**NEED TO DO UNDERSAMPLING**"""

Yes=data[data.Loan_Status==1]
No=data[data.Loan_Status==0]

No.shape

Yes.shape

Yes=Yes.sample(n=200)

Yes.shape

#NOW LETS CONCATINATE BOTH THE DATA
n_data=pd.concat([Yes,No],axis=0)

n_data.shape

n_data["Loan_Status"].value_counts()

#NOW WE DO SPLIT OF X AND Y DATA
X=n_data.drop(['Loan_Status'], axis=1,)
Y=n_data['Loan_Status']
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,random_state=2,stratify=Y)

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis

# List of classification models
classification_models = [
    LogisticRegression(),
    DecisionTreeClassifier(),
    RandomForestClassifier(),
    GradientBoostingClassifier(),
    SVC(),
    KNeighborsClassifier(),
    GaussianNB(),
    MLPClassifier(),
    LinearDiscriminantAnalysis(),
    QuadraticDiscriminantAnalysis()
]

# Now you can iterate over this list to access each classification model

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score
# Now you can iterate over this list to access each regression model

def Comparing_model():
  for model in classification_models:
    model.fit(X_train,Y_train)
    predicted_values=model.predict(X_test)
    accuracy=accuracy_score(Y_test,predicted_values)
    print("The accuracy of the",model,"is::",accuracy*100)

def Com():
  for i in classification_models:
    cv_sc=cross_val_score(i,X,Y,cv=5)
    acc=sum(cv_sc)/len(cv_sc)
    print("The accuracy score of is::",i,"::",acc)





n_data.info()
n_data.head()

unique_values_all_columns = n_data.apply(pd.Series.nunique)
print(unique_values_all_columns)

n_data.isnull().sum()

Com()

n_data.head()

n_data.tail()

n_data.info()

n_data.isnull().sum()

lr=LogisticRegression()
lr.fit(X_train,Y_train)

input_data=(1,2,1,1,2726,0,106,360,0,1)
#CHANGE THE INPUT DATA INTO ARRAY AND ITS IN TUPLE RIGHT NOW
input_data_numpy=np.asarray(input_data)
#RESHAPE THE NUMPY ARRAY AS WE ARE PREDICTION ONLY ONE DATA POINT
reshaped_data=input_data_numpy.reshape(1,-1)
prediction=lr.predict(reshaped_data)
if(prediction==1):
  print("Your Loan is approved")
else:
  print("Sorry your Loan is not approved")

n_data.head()
min(n_data['LoanAmount'])

import pickle
filename='trained_Loan_Model.sav'
pickle.dump(lr,open(filename,'wb'))

loaded_model=pickle.load(open('trained_Loan_Model.sav','rb'))

